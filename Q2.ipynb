{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d135d79",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f80a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd177e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5b79c",
   "metadata": {},
   "source": [
    "# Read From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af716e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(directory):\n",
    "    '''\n",
    "    Parameters:\n",
    "        directory: type(string)\n",
    "        \n",
    "    returns: list of all files in directory with the full path of file\n",
    "    '''\n",
    "    \n",
    "    list_of_files = []\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, file_path)\n",
    "        if os.path.isfile(full_path):\n",
    "            list_of_files.append(full_path)\n",
    "    \n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7c955",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b2062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "        data: type(string)\n",
    "    \n",
    "    returns: lowercase of data\n",
    "    '''\n",
    "    \n",
    "    return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d63a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_word_tokenize(corpus):\n",
    "    '''\n",
    "    Parameters:\n",
    "        corpus: type(string)\n",
    "    \n",
    "    returns word-level tokenization of corpus\n",
    "    '''\n",
    "    \n",
    "    return word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d844a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "        stopwords_set: type(set)\n",
    "    \n",
    "    returns: tokens without stopwords\n",
    "    '''\n",
    "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set]\n",
    "    \n",
    "    return tokens_sans_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15106da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without punctuation\n",
    "    '''\n",
    "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
    "    \n",
    "    return tokens_sans_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d3b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_space_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without blank tokens\n",
    "    '''\n",
    "    tokens_sans_blank_space = [x for x in tokens if x!='']\n",
    "    \n",
    "    return tokens_sans_blank_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254f18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: returns unique tokens after lemmatization\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    unique_lemmatize_tokens = list(dict.fromkeys(lemmatize_tokens))\n",
    "    \n",
    "    return unique_lemmatize_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06255586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, stopwords_set, preprocess_type):\n",
    "    # Convert the text to lower case\n",
    "    lowercase_corpus = lowercase(corpus)\n",
    "    #print(len(lowercase_corpus))\n",
    "    \n",
    "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
    "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
    "    #print(len(word_tokens))\n",
    "    \n",
    "    # Remove stopwords from tokens\n",
    "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
    "    #print(len(word_tokens_sans_stopwords))\n",
    "    \n",
    "    # Remove punctuation marks from tokens\n",
    "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
    "    #print(len(word_tokens_sans_punctuation))\n",
    "    \n",
    "    # Remove blank space tokens\n",
    "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
    "    #print(len(word_tokens_sans_blank_tokens))\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    #word_tokens_final = lemmatize_tokens(word_tokens_sans_blank_tokens)\n",
    "    #print(len(word_tokens_final))\n",
    "    \n",
    "    return word_tokens_sans_blank_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80392de7",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81ab55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_dictionary(list_of_files):\n",
    "    '''\n",
    "    Paramteres:\n",
    "        list_of_files: type(string)\n",
    "    \n",
    "    returns: file_dictionary with integer key and path_of_file as value\n",
    "    '''\n",
    "    file_dictionary = {}\n",
    "    for i in range(len(list_of_files)):\n",
    "        file_dictionary[i] = list_of_files[i]\n",
    "    \n",
    "    return file_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fffab6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_index(file_dictionary, stopwords_set):\n",
    "    # initialize positional index\n",
    "    positional_index = {}\n",
    "    \n",
    "    # positional index\n",
    "    for doc_ID in range(len(file_dictionary)):\n",
    "        file = open(file_dictionary[doc_ID], 'r', encoding='utf-8', errors='ignore')\n",
    "        file_corpus = file.read()\n",
    "        file.close()\n",
    "        doc_tokens = preprocess(file_corpus, stopwords_set, 'doc')\n",
    "        for j, word in enumerate(doc_tokens):\n",
    "            if(word in positional_index):\n",
    "                if(doc_ID in positional_index[word]):\n",
    "                    positional_index[word][doc_ID].append(j)\n",
    "                else:\n",
    "                    positional_index[word][doc_ID] = [j]\n",
    "            else:\n",
    "                positional_index[word] = {doc_ID:[j]}\n",
    "#         for token in doc_tokens:\n",
    "#             if token in positional_index:\n",
    "#                 positional_index[token][0] += 1\n",
    "#                 if doc_ID in positional_index[token][1]:\n",
    "#                     positional_index[token][1][doc_ID] = doc_tokens.index(token)\n",
    "#                 else:\n",
    "#                     positional_index[token][1] = SortedDict({doc_ID:positional_index[token]})\n",
    "#             else:\n",
    "#                 positional_index[token] = [1, SortedDict()]\n",
    "#                 positional_index[token][1] = SortedDict({doc_ID:positional_index[token]})\n",
    "    \n",
    "    # Storing positional index\n",
    "    pi_file = open('positional_index_pickle_file', 'wb')\n",
    "    pickle.dump(positional_index, pi_file)\n",
    "    pi_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496929b5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64cdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_keys(old_common_keys, new_list):\n",
    "    new_common_keys = [x for x in old_common_keys if x in new_list]\n",
    "    return new_common_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f726d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_positional_indexes(list1, list2):\n",
    "#     common_keys = [x for x in list1 if x in list2]\n",
    "#     common_dict = {}\n",
    "#     for ck in common_keys:\n",
    "#         if ck==27 or ck==209:\n",
    "#             common_dict[ck] = ['what','to','do']\n",
    "#     return common_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0521a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # create set of stop words for preprocessing\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    \n",
    "    # Get List of Files in Dataset\n",
    "    list_of_files = getListOfFiles('Dataset/Humor,Hist,Media,Food/')\n",
    "    \n",
    "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
    "    file_dictionary = create_file_dictionary(list_of_files)\n",
    "    \n",
    "    # create positional index once and then load pickle file afterwards\n",
    "    #create_positional_index(file_dictionary, stopwords_set)\n",
    "    \n",
    "    #Loading pre-processed files\n",
    "    pi_file = open('positional_index_pickle_file', 'rb')\n",
    "    positional_index = pickle.load(pi_file)\n",
    "    pi_file.close()\n",
    "    \n",
    "    N = int(input())\n",
    "    for q in range(N):\n",
    "        input_sentence = input()\n",
    "        sanitized_query = preprocess(input_sentence, stopwords_set, 'query')\n",
    "        print(sanitized_query)\n",
    "        query_size = len(sanitized_query)\n",
    "        \n",
    "        common_keys = [k for k in positional_index[sanitized_query[0]]]\n",
    "        for query_term in sanitized_query[1:]:\n",
    "            common_keys = get_common_keys(common_keys, positional_index[query_term])\n",
    "        \n",
    "        docs_satisfying_phrase_query = []\n",
    "        for ck in common_keys:\n",
    "            valid_indexes = positional_index[sanitized_query[0]][ck]\n",
    "            for amount in range(1,query_size):\n",
    "                next_indexes = [x-amount for x in positional_index[sanitized_query[amount]][ck]]\n",
    "                valid_indexes = [x for x in valid_indexes if x in next_indexes]\n",
    "            if len(valid_indexes)>0:\n",
    "                docs_satisfying_phrase_query.append(ck)\n",
    "        \n",
    "        print('Number of documents matched: {}'.format(len(docs_satisfying_phrase_query)))\n",
    "        print('Documents: \\n')\n",
    "        for doc_no in docs_satisfying_phrase_query:\n",
    "            print(file_dictionary[doc_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84a2f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "american dream\n",
      "['american', 'dream']\n",
      "Number of documents matched: 3\n",
      "Documents: \n",
      "\n",
      "Dataset/Humor,Hist,Media,Food/oliver.txt\n",
      "Dataset/Humor,Hist,Media,Food/oliver02.txt\n",
      "Dataset/Humor,Hist,Media,Food/p-law.hum\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
