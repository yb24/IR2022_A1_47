{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d135d79",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f80a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd177e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5b79c",
   "metadata": {},
   "source": [
    "# Read From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af716e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(directory):\n",
    "    '''\n",
    "    Parameters:\n",
    "        directory: type(string)\n",
    "        \n",
    "    returns: list of all files in directory with the full path of file\n",
    "    '''\n",
    "    \n",
    "    list_of_files = []\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, file_path)\n",
    "        if os.path.isfile(full_path):\n",
    "            list_of_files.append(full_path)\n",
    "    \n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7c955",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b2062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "        data: type(string)\n",
    "    \n",
    "    returns: lowercase of data\n",
    "    '''\n",
    "    \n",
    "    return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d63a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_word_tokenize(corpus):\n",
    "    '''\n",
    "    Parameters:\n",
    "        corpus: type(string)\n",
    "    \n",
    "    returns word-level tokenization of corpus\n",
    "    '''\n",
    "    \n",
    "    return word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d844a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "        stopwords_set: type(set)\n",
    "    \n",
    "    returns: tokens without stopwords\n",
    "    '''\n",
    "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set]\n",
    "    \n",
    "    return tokens_sans_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15106da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without punctuation\n",
    "    '''\n",
    "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
    "    \n",
    "    return tokens_sans_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d3b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_space_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without blank tokens\n",
    "    '''\n",
    "    tokens_sans_blank_space = [x for x in tokens if x!='']\n",
    "    \n",
    "    return tokens_sans_blank_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254f18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: returns unique tokens after lemmatization\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    unique_lemmatize_tokens = list(dict.fromkeys(lemmatize_tokens))\n",
    "    \n",
    "    return unique_lemmatize_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06255586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, stopwords_set, preprocess_type):\n",
    "    # Convert the text to lower case\n",
    "    lowercase_corpus = lowercase(corpus)\n",
    "    #print(len(lowercase_corpus))\n",
    "    \n",
    "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
    "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
    "    #print(len(word_tokens))\n",
    "    \n",
    "    # Remove stopwords from tokens\n",
    "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
    "    #print(len(word_tokens_sans_stopwords))\n",
    "    \n",
    "    # Remove punctuation marks from tokens\n",
    "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
    "    #print(len(word_tokens_sans_punctuation))\n",
    "    \n",
    "    # Remove blank space tokens\n",
    "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
    "    #print(len(word_tokens_sans_blank_tokens))\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    #word_tokens_final = lemmatize_tokens(word_tokens_sans_blank_tokens)\n",
    "    #print(len(word_tokens_final))\n",
    "    \n",
    "    return word_tokens_sans_blank_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80392de7",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81ab55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_dictionary(list_of_files):\n",
    "    '''\n",
    "    Paramteres:\n",
    "        list_of_files: type(string)\n",
    "    \n",
    "    returns: file_dictionary with integer key and path_of_file as value\n",
    "    '''\n",
    "    file_dictionary = {}\n",
    "    for i in range(len(list_of_files)):\n",
    "        file_dictionary[i] = list_of_files[i]\n",
    "    \n",
    "    return file_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fffab6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_index(file_dictionary, stopwords_set):\n",
    "    # initialize positional index\n",
    "    positional_index = SortedDict()\n",
    "    \n",
    "    # positional index\n",
    "    for doc_ID in range(1):\n",
    "        file = open(file_dictionary[doc_ID], 'r', encoding='utf-8', errors='ignore')\n",
    "        file_corpus = file.read()\n",
    "        file.close()\n",
    "        doc_tokens = preprocess(file_corpus, stopwords_set, 'doc')\n",
    "        print(doc_tokens)\n",
    "        for index in range(len(doc_tokens)):\n",
    "            print(doc_tokens[index])\n",
    "            if doc_tokens[index] in positional_index:\n",
    "                continue\n",
    "            else:\n",
    "                positional_index = [1, {doc_ID:[index]}]\n",
    "#         for token in doc_tokens:\n",
    "#             if token in positional_index:\n",
    "#                 positional_index[token][0] += 1\n",
    "#                 if doc_ID in positional_index[token][1]:\n",
    "#                     positional_index[token][1][doc_ID] = doc_tokens.index(token)\n",
    "#                 else:\n",
    "#                     positional_index[token][1] = SortedDict({doc_ID:positional_index[token]})\n",
    "#             else:\n",
    "#                 positional_index[token] = [1, SortedDict()]\n",
    "#                 positional_index[token][1] = SortedDict({doc_ID:positional_index[token]})\n",
    "    \n",
    "    print(positional_index)\n",
    "    # Storing positional index\n",
    "    pi_file = open('positional_index_pickle_file', 'wb')\n",
    "    pickle.dump(positional_index, pi_file)\n",
    "    pi_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496929b5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0521a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # create set of stop words for preprocessing\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    \n",
    "    # Get List of Files in Dataset\n",
    "    list_of_files = getListOfFiles('Dataset/Humor,Hist,Media,Food/')\n",
    "    \n",
    "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
    "    file_dictionary = create_file_dictionary(list_of_files)\n",
    "    \n",
    "    # create positional index once and then load pickle file afterwards\n",
    "    create_positional_index(file_dictionary, stopwords_set)\n",
    "    \n",
    "    #Loading pre-processed files\n",
    "    pi_file = open('positional_index_pickle_file', 'rb')\n",
    "    pi_inverted_index = pickle.load(pi_file)\n",
    "    pi_file.close()\n",
    "    \n",
    "    print(pi_inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84a2f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['herbalherb1st', 'aidcalendulacomfreyremediessickmedicine', 'herbal', 'first', 'aid', 'kit', 'herbal', 'first', 'aid', 'kit', 'calendula', 'ointment', 'use', 'minor', 'cuts', 'grazes', 'red', 'rashes', 'minor', 'skin', 'rash', 'comfrey', 'ointment', 'suitable', 'bruises', 'minor', 'damage', 'external', 'blood', 'vessels', 'veins', 'st', 'johnswort', 'oil', 'beneficial', 'itchy', 'skin', 'irritable', 'psoriasis', 'also', 'good', 'sunburn', 'applied', 'night', 'liver', 'mixture', 'mild', 'laxative', 'properties', 'helps', 'digestion', 'rich', 'food', 'take', 'one', 'teaspoon', 'night', '30', 'minutes', 'main', 'meal', 'parasite', 'mixture', 'effective', 'common', 'internal', 'parasites', 'infestation', 'suspected', 'abstain', 'food', '24', 'hours', 'take', 'one', 'tablespoon', 'mixture', 'little', 'water', 'repeat', 'dose', 'four', 'hours', 'another', 'four', 'hours', 'parasites', 'died', 'able', 'recommence', 'eating', 'four', 'hours', 'last', 'dose', 'gasp', 'may', 'also', 'used', 'skin', 'wash', 'external', 'parasites', 'nervine', 'sedative', 'mixture', 'take', '25', 'drops', '3', 'x', 'daily', 'empty', 'stomach', 'general', 'sedative', 'trouble', 'sleeping', 'night', 'take', 'one', 'teaspoon', 'little', 'water', '30', 'minutes', 'bedtime', 'astringent', 'mix', 'good', 'internal', 'bleeding', 'also', 'effective', 'remedy', 'occasional', 'diarrhoea', 'stricken', 'runs', 'take', 'one', 'teaspoonful', 'little', 'water', 'every', 'two', 'hours', 'symptoms', 'subside', 'follow', 'echinacea', 'goldenseal', 'tincture', 'echinacea', 'goldenseal', 'similar', 'effect', 'antibiotic', 'use', 'event', 'serious', 'infection', 'etc', 'take', '25', 'drops', 'little', 'water', '4', 'x', 'daily', 'half', 'hour', 'meals', 'continue', 'least', 'two', 'weeks', 'may', 'de', 'used', 'externally', 'antiseptic', 'anaesthetic', 'lotion', 'echinacea', 'tincture', 'similar', 'previous', 'mixture', 'suitable', 'use', 'long', 'period', 'taken', 'internally', 'may', 'taken', 'one', 'month', 'order', 'boost', 'overall', 'effectiveness', 'immune', 'system', 'important', 'remedies', 'way', 'intended', 'substitute', 'proper', 'medical', 'care', 'attention', 'symptoms', 'persist', 'please', 'consult', 'reputable', 'health', 'care', 'practitioner', '\\x1a']\n",
      "herbalherb1st\n",
      "aidcalendulacomfreyremediessickmedicine\n",
      "herbal\n",
      "first\n",
      "aid\n",
      "kit\n",
      "herbal\n",
      "first\n",
      "aid\n",
      "kit\n",
      "calendula\n",
      "ointment\n",
      "use\n",
      "minor\n",
      "cuts\n",
      "grazes\n",
      "red\n",
      "rashes\n",
      "minor\n",
      "skin\n",
      "rash\n",
      "comfrey\n",
      "ointment\n",
      "suitable\n",
      "bruises\n",
      "minor\n",
      "damage\n",
      "external\n",
      "blood\n",
      "vessels\n",
      "veins\n",
      "st\n",
      "johnswort\n",
      "oil\n",
      "beneficial\n",
      "itchy\n",
      "skin\n",
      "irritable\n",
      "psoriasis\n",
      "also\n",
      "good\n",
      "sunburn\n",
      "applied\n",
      "night\n",
      "liver\n",
      "mixture\n",
      "mild\n",
      "laxative\n",
      "properties\n",
      "helps\n",
      "digestion\n",
      "rich\n",
      "food\n",
      "take\n",
      "one\n",
      "teaspoon\n",
      "night\n",
      "30\n",
      "minutes\n",
      "main\n",
      "meal\n",
      "parasite\n",
      "mixture\n",
      "effective\n",
      "common\n",
      "internal\n",
      "parasites\n",
      "infestation\n",
      "suspected\n",
      "abstain\n",
      "food\n",
      "24\n",
      "hours\n",
      "take\n",
      "one\n",
      "tablespoon\n",
      "mixture\n",
      "little\n",
      "water\n",
      "repeat\n",
      "dose\n",
      "four\n",
      "hours\n",
      "another\n",
      "four\n",
      "hours\n",
      "parasites\n",
      "died\n",
      "able\n",
      "recommence\n",
      "eating\n",
      "four\n",
      "hours\n",
      "last\n",
      "dose\n",
      "gasp\n",
      "may\n",
      "also\n",
      "used\n",
      "skin\n",
      "wash\n",
      "external\n",
      "parasites\n",
      "nervine\n",
      "sedative\n",
      "mixture\n",
      "take\n",
      "25\n",
      "drops\n",
      "3\n",
      "x\n",
      "daily\n",
      "empty\n",
      "stomach\n",
      "general\n",
      "sedative\n",
      "trouble\n",
      "sleeping\n",
      "night\n",
      "take\n",
      "one\n",
      "teaspoon\n",
      "little\n",
      "water\n",
      "30\n",
      "minutes\n",
      "bedtime\n",
      "astringent\n",
      "mix\n",
      "good\n",
      "internal\n",
      "bleeding\n",
      "also\n",
      "effective\n",
      "remedy\n",
      "occasional\n",
      "diarrhoea\n",
      "stricken\n",
      "runs\n",
      "take\n",
      "one\n",
      "teaspoonful\n",
      "little\n",
      "water\n",
      "every\n",
      "two\n",
      "hours\n",
      "symptoms\n",
      "subside\n",
      "follow\n",
      "echinacea\n",
      "goldenseal\n",
      "tincture\n",
      "echinacea\n",
      "goldenseal\n",
      "similar\n",
      "effect\n",
      "antibiotic\n",
      "use\n",
      "event\n",
      "serious\n",
      "infection\n",
      "etc\n",
      "take\n",
      "25\n",
      "drops\n",
      "little\n",
      "water\n",
      "4\n",
      "x\n",
      "daily\n",
      "half\n",
      "hour\n",
      "meals\n",
      "continue\n",
      "least\n",
      "two\n",
      "weeks\n",
      "may\n",
      "de\n",
      "used\n",
      "externally\n",
      "antiseptic\n",
      "anaesthetic\n",
      "lotion\n",
      "echinacea\n",
      "tincture\n",
      "similar\n",
      "previous\n",
      "mixture\n",
      "suitable\n",
      "use\n",
      "long\n",
      "period\n",
      "taken\n",
      "internally\n",
      "may\n",
      "taken\n",
      "one\n",
      "month\n",
      "order\n",
      "boost\n",
      "overall\n",
      "effectiveness\n",
      "immune\n",
      "system\n",
      "important\n",
      "remedies\n",
      "way\n",
      "intended\n",
      "substitute\n",
      "proper\n",
      "medical\n",
      "care\n",
      "attention\n",
      "symptoms\n",
      "persist\n",
      "please\n",
      "consult\n",
      "reputable\n",
      "health\n",
      "care\n",
      "practitioner\n",
      "\u001a\n",
      "[1, {0: [223]}]\n",
      "[1, {0: [223]}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
